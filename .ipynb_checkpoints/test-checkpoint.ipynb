{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7024ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mayal\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mayal\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import (Dense, Flatten, Reshape, Concatenate, Conv2D,\n",
    "                                     UpSampling2D, BatchNormalization, MaxPooling2D, Conv2DTranspose)\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50fd8f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_prior(num_modes, latent_dim):\n",
    "    \"\"\"\n",
    "    This function should create an instance of a MixtureSameFamily distribution\n",
    "    according to the above specification.\n",
    "    The function takes the num_modes and latent_dim as arguments, which should\n",
    "    be used to define the distribution.\n",
    "    Your function should then return the distribution instance.\n",
    "    \"\"\"\n",
    "    gm = tfp.distributions.MixtureSameFamily(\n",
    "        # the mixture_distribution should be fixed to a uniform\n",
    "        # tfd.Categorical distribution, so that  pik=1/K  in the above equation.\n",
    "        # This argument will therefore not contain any trainable variables\n",
    "        mixture_distribution=tfp.distributions.Categorical(\n",
    "            probs=[1.0/num_modes,]*num_modes),\n",
    "\n",
    "        # The components_distribution should be a tfd.MultivariateNormalDiag\n",
    "        # distribution batch shape equal to [num_modes] and event shape equal to [latent_dim].\n",
    "        components_distribution = tfp.distributions.MultivariateNormalDiag(\n",
    "          # should have trainable loc parameter (initialised with a random normal distribution)\n",
    "          loc = tf.Variable(tf.random.normal(shape = [num_modes, latent_dim])),\n",
    "\n",
    "          # and trainable scale_diag parameter (initialised to ones)\n",
    "          # The scale_diag variable should be enforced to be positive using\n",
    "          # tfp.util.TransformedVariable and the tfb.Softplus bijection\n",
    "          scale_diag = tfp.util.TransformedVariable(\n",
    "                                                tf.Variable(\n",
    "                                                  tf.ones(shape = [num_modes, latent_dim])),\n",
    "                                                bijector = tfp.bijectors.Softplus()\n",
    "                                                )\n",
    "        )\n",
    "      )\n",
    "\n",
    "\n",
    "    return gm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b3a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the prior distribution with 2 components and latent_dim = 50\n",
    "\n",
    "prior = get_prior(num_modes=2, latent_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08823cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl_regularizer(prior_distribution):\n",
    "    \"\"\"\n",
    "    This function should create an instance of the KLDivergenceRegularizer\n",
    "    according to the above specification.\n",
    "    The function takes the prior_distribution, which should be used to define\n",
    "    the distribution.\n",
    "    Your function should then return the KLDivergenceRegularizer instance.\n",
    "    \"\"\"\n",
    "    reg = tfp.layers.KLDivergenceRegularizer(\n",
    "        prior_distribution,\n",
    "        weight = 1.0,\n",
    "        use_exact_kl = False,\n",
    "        test_points_fn = lambda q : q.sample(3),\n",
    "        test_points_reduce_axis = (0,1))\n",
    "\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ae457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_regularizer = get_kl_regularizer(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a28ab087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_encoder(latent_dim, kl_regularizer):\n",
    "    \"\"\"\n",
    "    This function should build a CNN encoder model according to the above specification.\n",
    "    The function takes latent_dim and kl_regularizer as arguments, which should be\n",
    "    used to define the model.\n",
    "    Your function should return the encoder model.\n",
    "    \"\"\"\n",
    "    input_shape = (1024,2048,1)\n",
    "    encoder = Sequential([\n",
    "        Conv2D(filters = 32, kernel_size = 4, activation = 'relu',\n",
    "               strides = (2,4), padding = 'SAME', input_shape = input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 4), strides=(2, 4), padding='SAME'), \n",
    "        \n",
    "        Conv2D(filters = 64, kernel_size = 4, activation = 'relu',\n",
    "               strides = (2,4), padding = 'SAME'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 4), strides=(2, 4), padding='SAME'),  \n",
    "        \n",
    "        Conv2D(filters = 128, kernel_size = 4, activation = 'relu',\n",
    "               strides = (2,4), padding = 'SAME'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 4), strides=(2, 4), padding='SAME'),  \n",
    "\n",
    "        Conv2D(filters = 256, kernel_size = 4, activation = 'relu',\n",
    "               strides = (2,4), padding = 'SAME'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 4), strides=(2, 4), padding='SAME'),  \n",
    "\n",
    "        Flatten(),\n",
    "        Dense(tfp.layers.MultivariateNormalTriL.params_size(latent_dim)),\n",
    "        tfp.layers.MultivariateNormalTriL(latent_dim, activity_regularizer = kl_regularizer)\n",
    "    ])\n",
    "\n",
    "    return encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3831abe4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = get_encoder(latent_dim=100, kl_regularizer=kl_regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64fd931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 512, 512, 32)      544       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 512, 512, 32)      128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 256, 128, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 128, 32, 64)       32832     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 128, 32, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 64, 8, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 2, 128)        131200    \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 32, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 1, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 1, 256)         524544    \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 8, 1, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 4, 1, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5150)              5278750   \n",
      "                                                                 \n",
      " multivariate_normal_tri_l   ((None, 100),             400       \n",
      " (MultivariateNormalTriL)     (None, 100))                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5970190 (22.77 MB)\n",
      "Trainable params: 5969230 (22.77 MB)\n",
      "Non-trainable params: 960 (3.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc33ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder(latent_dim):\n",
    "    decoder = Sequential([\n",
    "        Dense(1024, activation='relu', input_shape=(latent_dim,)),\n",
    "        Reshape((4, 1, 256)),\n",
    "        Conv2DTranspose(256, kernel_size=3, strides=(2, 4), padding='SAME', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Conv2DTranspose(128, kernel_size=3, strides=(2, 4), padding='SAME', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv2DTranspose(64, kernel_size=3, strides=(4, 4), padding='SAME', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv2DTranspose(32, kernel_size=3, strides=(4, 4), padding='SAME', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv2DTranspose(1, kernel_size=3, strides=(4, 8), padding='SAME', activation='sigmoid'),\n",
    "        Flatten(),\n",
    "        tfp.layers.IndependentBernoulli(event_shape = (1024,2048,1))\n",
    "    ])\n",
    "\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5c877a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = get_decoder(latent_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929e8ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 1024)              103424    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 4, 1, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 8, 4, 256)         590080    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 8, 4, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 16, 16, 128)       295040    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 16, 16, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 64, 64, 64)        73792     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 64, 64, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2D  (None, 256, 256, 32)      18464     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 256, 256, 32)      128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2D  (None, 1024, 2048, 1)     289       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2097152)           0         \n",
      "                                                                 \n",
      " independent_bernoulli (Ind  ((None, 1024, 2048, 1),   0         \n",
      " ependentBernoulli)           (None, 1024, 2048, 1))             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1083009 (4.13 MB)\n",
      "Trainable params: 1082049 (4.13 MB)\n",
      "Non-trainable params: 960 (3.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed51f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reconstruction_loss(batch_of_images, decoding_dist):\n",
    "    \"\"\"\n",
    "    This function should compute and return the average expected reconstruction loss,\n",
    "    as defined above.\n",
    "    The function takes batch_of_spectograms (Tensor containing a batch of input spectogram data) and decoding_dist (output distribution of decoder after passing the\n",
    "    spectogram batch through the encoder and decoder) as arguments.\n",
    "    The function should return the scalar average expected reconstruction loss.\n",
    "    \"\"\"\n",
    "    return -tf.reduce_sum(decoding_dist.log_prob(batch_of_images), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc67ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Model(inputs=encoder.inputs, outputs=decoder(encoder.outputs))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "vae.compile(optimizer=optimizer, loss=reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1e2ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('stft_spectrograms.npz')\n",
    "loaded_spectrograms_2 = np.array([data[f'{i}'] for i in range(len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e07c9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100\n"
     ]
    }
   ],
   "source": [
    "train_data = loaded_spectrograms_2[:1100]\n",
    "val_data = loaded_spectrograms_2[1100:]\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8fae628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:From C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "68/68 [==============================] - ETA: 0s - loss: 28817364.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\saving\\serialization_lib.py:159: UserWarning: The object being serialized includes a `lambda`. This is unsafe. In order to reload the object, you will have to pass `safe_mode=False` to the loading function. Please avoid using `lambda` in the future, and use named Python functions instead. This is the `lambda` being serialized:         lambda t: MultivariateNormalTriL.new(t, event_size, validate_args),\n",
      "\n",
      "  config_arr = [serialize_keras_object(x) for x in obj]\n",
      "C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\saving\\serialization_lib.py:159: UserWarning: The object being serialized includes a `lambda`. This is unsafe. In order to reload the object, you will have to pass `safe_mode=False` to the loading function. Please avoid using `lambda` in the future, and use named Python functions instead. This is the `lambda` being serialized:         lambda t: IndependentBernoulli.new(  # pylint: disable=g-long-lambda\n",
      "            t, event_shape, sample_dtype, validate_args),\n",
      "\n",
      "  config_arr = [serialize_keras_object(x) for x in obj]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 162s 2s/step - loss: 28817364.0000 - val_loss: 29240886.0000\n",
      "Epoch 2/150\n",
      "68/68 [==============================] - 164s 2s/step - loss: 28252122.0000 - val_loss: 28637584.0000\n",
      "Epoch 3/150\n",
      "68/68 [==============================] - 174s 3s/step - loss: 27797882.0000 - val_loss: 28064716.0000\n",
      "Epoch 4/150\n",
      "68/68 [==============================] - 173s 3s/step - loss: 27510516.0000 - val_loss: 27780930.0000\n",
      "Epoch 5/150\n",
      "68/68 [==============================] - 826s 12s/step - loss: 27318512.0000 - val_loss: 27610960.0000\n",
      "Epoch 6/150\n",
      "68/68 [==============================] - 166s 2s/step - loss: 27159356.0000 - val_loss: 27477736.0000\n",
      "Epoch 7/150\n",
      "68/68 [==============================] - 166s 2s/step - loss: 27040408.0000 - val_loss: 27318426.0000\n",
      "Epoch 8/150\n",
      "68/68 [==============================] - 169s 2s/step - loss: 26917712.0000 - val_loss: 27220024.0000\n",
      "Epoch 9/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 26802932.0000 - val_loss: 27149200.0000\n",
      "Epoch 10/150\n",
      "68/68 [==============================] - 163s 2s/step - loss: 26707404.0000 - val_loss: 27056134.0000\n",
      "Epoch 11/150\n",
      "68/68 [==============================] - 168s 2s/step - loss: 26625972.0000 - val_loss: 26953688.0000\n",
      "Epoch 12/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 26540162.0000 - val_loss: 26873474.0000\n",
      "Epoch 13/150\n",
      "68/68 [==============================] - 274s 4s/step - loss: 26461452.0000 - val_loss: 26797300.0000\n",
      "Epoch 14/150\n",
      "68/68 [==============================] - 165s 2s/step - loss: 26391814.0000 - val_loss: 26724590.0000\n",
      "Epoch 15/150\n",
      "68/68 [==============================] - 167s 2s/step - loss: 26312302.0000 - val_loss: 26652620.0000\n",
      "Epoch 16/150\n",
      "68/68 [==============================] - 173s 3s/step - loss: 26252772.0000 - val_loss: 26581584.0000\n",
      "Epoch 17/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 26182988.0000 - val_loss: 26518732.0000\n",
      "Epoch 18/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 26114612.0000 - val_loss: 26443320.0000\n",
      "Epoch 19/150\n",
      "68/68 [==============================] - 226s 3s/step - loss: 26059236.0000 - val_loss: 26374702.0000\n",
      "Epoch 20/150\n",
      "68/68 [==============================] - 165s 2s/step - loss: 25999646.0000 - val_loss: 26307938.0000\n",
      "Epoch 21/150\n",
      "68/68 [==============================] - 178s 3s/step - loss: 25938328.0000 - val_loss: 26243246.0000\n",
      "Epoch 22/150\n",
      "68/68 [==============================] - 172s 3s/step - loss: 25880078.0000 - val_loss: 26178452.0000\n",
      "Epoch 23/150\n",
      "68/68 [==============================] - 170s 3s/step - loss: 25817676.0000 - val_loss: 26114874.0000\n",
      "Epoch 24/150\n",
      "68/68 [==============================] - 169s 2s/step - loss: 25761160.0000 - val_loss: 26052882.0000\n",
      "Epoch 25/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 25705762.0000 - val_loss: 25992308.0000\n",
      "Epoch 26/150\n",
      "68/68 [==============================] - 169s 2s/step - loss: 25653404.0000 - val_loss: 25932580.0000\n",
      "Epoch 27/150\n",
      "68/68 [==============================] - 166s 2s/step - loss: 25601432.0000 - val_loss: 25873726.0000\n",
      "Epoch 28/150\n",
      "68/68 [==============================] - 168s 2s/step - loss: 25536530.0000 - val_loss: 25815846.0000\n",
      "Epoch 29/150\n",
      "68/68 [==============================] - 539s 8s/step - loss: 25499222.0000 - val_loss: 25759874.0000\n",
      "Epoch 30/150\n",
      "68/68 [==============================] - 165s 2s/step - loss: 25442378.0000 - val_loss: 25704932.0000\n",
      "Epoch 31/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 25392266.0000 - val_loss: 25651874.0000\n",
      "Epoch 32/150\n",
      "68/68 [==============================] - 164s 2s/step - loss: 25345660.0000 - val_loss: 25598078.0000\n",
      "Epoch 33/150\n",
      "68/68 [==============================] - 164s 2s/step - loss: 25293968.0000 - val_loss: 25546092.0000\n",
      "Epoch 34/150\n",
      "68/68 [==============================] - 1024s 15s/step - loss: 25253726.0000 - val_loss: 25495842.0000\n",
      "Epoch 35/150\n",
      "68/68 [==============================] - 163s 2s/step - loss: 25203152.0000 - val_loss: 25446458.0000\n",
      "Epoch 36/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 25160760.0000 - val_loss: 25397842.0000\n",
      "Epoch 37/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 25106672.0000 - val_loss: 25350262.0000\n",
      "Epoch 38/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 25063446.0000 - val_loss: 25304128.0000\n",
      "Epoch 39/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 25021126.0000 - val_loss: 25258982.0000\n",
      "Epoch 40/150\n",
      "68/68 [==============================] - 157s 2s/step - loss: 24981624.0000 - val_loss: 25214668.0000\n",
      "Epoch 41/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 24939984.0000 - val_loss: 25171146.0000\n",
      "Epoch 42/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 24904384.0000 - val_loss: 25128586.0000\n",
      "Epoch 43/150\n",
      "68/68 [==============================] - 192s 3s/step - loss: 24864262.0000 - val_loss: 25087394.0000\n",
      "Epoch 44/150\n",
      "68/68 [==============================] - 198s 3s/step - loss: 24827576.0000 - val_loss: 25047152.0000\n",
      "Epoch 45/150\n",
      "68/68 [==============================] - 187s 3s/step - loss: 24792932.0000 - val_loss: 25007540.0000\n",
      "Epoch 46/150\n",
      "68/68 [==============================] - 179s 3s/step - loss: 24758470.0000 - val_loss: 24968688.0000\n",
      "Epoch 47/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 24727342.0000 - val_loss: 24930824.0000\n",
      "Epoch 48/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 24689416.0000 - val_loss: 24894008.0000\n",
      "Epoch 49/150\n",
      "68/68 [==============================] - 164s 2s/step - loss: 24654974.0000 - val_loss: 24858138.0000\n",
      "Epoch 50/150\n",
      "68/68 [==============================] - 168s 2s/step - loss: 24619550.0000 - val_loss: 24822918.0000\n",
      "Epoch 51/150\n",
      "68/68 [==============================] - 167s 2s/step - loss: 24587078.0000 - val_loss: 24788326.0000\n",
      "Epoch 52/150\n",
      "68/68 [==============================] - 164s 2s/step - loss: 24557284.0000 - val_loss: 24754560.0000\n",
      "Epoch 53/150\n",
      "68/68 [==============================] - 186s 3s/step - loss: 24530316.0000 - val_loss: 24721768.0000\n",
      "Epoch 54/150\n",
      "68/68 [==============================] - 168s 2s/step - loss: 24504540.0000 - val_loss: 24734710.0000\n",
      "Epoch 55/150\n",
      "68/68 [==============================] - 168s 2s/step - loss: 24473154.0000 - val_loss: 24865692.0000\n",
      "Epoch 56/150\n",
      "68/68 [==============================] - 185s 3s/step - loss: 24445342.0000 - val_loss: 24650404.0000\n",
      "Epoch 57/150\n",
      "68/68 [==============================] - 163s 2s/step - loss: 24416932.0000 - val_loss: 24599290.0000\n",
      "Epoch 58/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 24388246.0000 - val_loss: 24568832.0000\n",
      "Epoch 59/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 24364704.0000 - val_loss: 24540638.0000\n",
      "Epoch 60/150\n",
      "68/68 [==============================] - 164s 2s/step - loss: 24336366.0000 - val_loss: 24512348.0000\n",
      "Epoch 61/150\n",
      "68/68 [==============================] - 168s 2s/step - loss: 24312610.0000 - val_loss: 24484882.0000\n",
      "Epoch 62/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 24285092.0000 - val_loss: 24458828.0000\n",
      "Epoch 63/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 24261220.0000 - val_loss: 24432092.0000\n",
      "Epoch 64/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 24235766.0000 - val_loss: 24406576.0000\n",
      "Epoch 65/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 24217414.0000 - val_loss: 24381580.0000\n",
      "Epoch 66/150\n",
      "68/68 [==============================] - 170s 3s/step - loss: 24192388.0000 - val_loss: 24379646.0000\n",
      "Epoch 67/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 24171912.0000 - val_loss: 24353030.0000\n",
      "Epoch 68/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 24149296.0000 - val_loss: 24310876.0000\n",
      "Epoch 69/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 24132488.0000 - val_loss: 24287734.0000\n",
      "Epoch 70/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 24178328.0000 - val_loss: 24265610.0000\n",
      "Epoch 71/150\n",
      "68/68 [==============================] - 168s 2s/step - loss: 24091904.0000 - val_loss: 24243766.0000\n",
      "Epoch 72/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 166s 2s/step - loss: 24074438.0000 - val_loss: 24222456.0000\n",
      "Epoch 73/150\n",
      "68/68 [==============================] - 163s 2s/step - loss: 24054352.0000 - val_loss: 24201822.0000\n",
      "Epoch 74/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 24035572.0000 - val_loss: 24181570.0000\n",
      "Epoch 75/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 24014930.0000 - val_loss: 24161704.0000\n",
      "Epoch 76/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23999564.0000 - val_loss: 24142326.0000\n",
      "Epoch 77/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 23981552.0000 - val_loss: 24123528.0000\n",
      "Epoch 78/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23962098.0000 - val_loss: 24105070.0000\n",
      "Epoch 79/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 23944506.0000 - val_loss: 24086940.0000\n",
      "Epoch 80/150\n",
      "68/68 [==============================] - 157s 2s/step - loss: 23929558.0000 - val_loss: 24069296.0000\n",
      "Epoch 81/150\n",
      "68/68 [==============================] - 165s 2s/step - loss: 23913008.0000 - val_loss: 24052112.0000\n",
      "Epoch 82/150\n",
      "68/68 [==============================] - 158s 2s/step - loss: 23897314.0000 - val_loss: 24035304.0000\n",
      "Epoch 83/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23883100.0000 - val_loss: 24018742.0000\n",
      "Epoch 84/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 23866270.0000 - val_loss: 24002542.0000\n",
      "Epoch 85/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 23854180.0000 - val_loss: 23986808.0000\n",
      "Epoch 86/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23839380.0000 - val_loss: 23971454.0000\n",
      "Epoch 87/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23824906.0000 - val_loss: 23956346.0000\n",
      "Epoch 88/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23813562.0000 - val_loss: 23941530.0000\n",
      "Epoch 89/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23800986.0000 - val_loss: 23927124.0000\n",
      "Epoch 90/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23788164.0000 - val_loss: 23913052.0000\n",
      "Epoch 91/150\n",
      "68/68 [==============================] - 158s 2s/step - loss: 23776016.0000 - val_loss: 23899270.0000\n",
      "Epoch 92/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23762916.0000 - val_loss: 23885734.0000\n",
      "Epoch 93/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23751082.0000 - val_loss: 23872492.0000\n",
      "Epoch 94/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23739610.0000 - val_loss: 23859584.0000\n",
      "Epoch 95/150\n",
      "68/68 [==============================] - 158s 2s/step - loss: 23728718.0000 - val_loss: 23847002.0000\n",
      "Epoch 96/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23718496.0000 - val_loss: 23834676.0000\n",
      "Epoch 97/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23704290.0000 - val_loss: 23822552.0000\n",
      "Epoch 98/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23697536.0000 - val_loss: 23810740.0000\n",
      "Epoch 99/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 23685080.0000 - val_loss: 23799202.0000\n",
      "Epoch 100/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23674696.0000 - val_loss: 23787852.0000\n",
      "Epoch 101/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 23665210.0000 - val_loss: 23776832.0000\n",
      "Epoch 102/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 23654524.0000 - val_loss: 23766060.0000\n",
      "Epoch 103/150\n",
      "68/68 [==============================] - 163s 2s/step - loss: 23646896.0000 - val_loss: 23755466.0000\n",
      "Epoch 104/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 23635774.0000 - val_loss: 23745080.0000\n",
      "Epoch 105/150\n",
      "68/68 [==============================] - 164s 2s/step - loss: 23627172.0000 - val_loss: 23735000.0000\n",
      "Epoch 106/150\n",
      "68/68 [==============================] - 163s 2s/step - loss: 23615690.0000 - val_loss: 23725136.0000\n",
      "Epoch 107/150\n",
      "68/68 [==============================] - 163s 2s/step - loss: 23606710.0000 - val_loss: 23715390.0000\n",
      "Epoch 108/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23597994.0000 - val_loss: 23706022.0000\n",
      "Epoch 109/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 23589940.0000 - val_loss: 23696728.0000\n",
      "Epoch 110/150\n",
      "68/68 [==============================] - 163s 2s/step - loss: 23581272.0000 - val_loss: 23687628.0000\n",
      "Epoch 111/150\n",
      "68/68 [==============================] - 164s 2s/step - loss: 23574026.0000 - val_loss: 23678756.0000\n",
      "Epoch 112/150\n",
      "68/68 [==============================] - 158s 2s/step - loss: 23565642.0000 - val_loss: 23670124.0000\n",
      "Epoch 113/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23558090.0000 - val_loss: 23661648.0000\n",
      "Epoch 114/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 23551010.0000 - val_loss: 23653288.0000\n",
      "Epoch 115/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23543774.0000 - val_loss: 23645176.0000\n",
      "Epoch 116/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 23538024.0000 - val_loss: 23637260.0000\n",
      "Epoch 117/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 23529516.0000 - val_loss: 23629434.0000\n",
      "Epoch 118/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23522216.0000 - val_loss: 23621780.0000\n",
      "Epoch 119/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 23519016.0000 - val_loss: 23644336.0000\n",
      "Epoch 120/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 23509812.0000 - val_loss: 23610092.0000\n",
      "Epoch 121/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 23502716.0000 - val_loss: 23600102.0000\n",
      "Epoch 122/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23492418.0000 - val_loss: 23583508.0000\n",
      "Epoch 123/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 23482290.0000 - val_loss: 23575596.0000\n",
      "Epoch 124/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23474034.0000 - val_loss: 23568524.0000\n",
      "Epoch 125/150\n",
      "68/68 [==============================] - 164s 2s/step - loss: 23468190.0000 - val_loss: 23561680.0000\n",
      "Epoch 126/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23462012.0000 - val_loss: 23555252.0000\n",
      "Epoch 127/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23455612.0000 - val_loss: 23548702.0000\n",
      "Epoch 128/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23450486.0000 - val_loss: 23542536.0000\n",
      "Epoch 129/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23444322.0000 - val_loss: 23536460.0000\n",
      "Epoch 130/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 23439258.0000 - val_loss: 23530544.0000\n",
      "Epoch 131/150\n",
      "68/68 [==============================] - 161s 2s/step - loss: 23433394.0000 - val_loss: 23524760.0000\n",
      "Epoch 132/150\n",
      "68/68 [==============================] - 163s 2s/step - loss: 23428130.0000 - val_loss: 23518996.0000\n",
      "Epoch 133/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23422494.0000 - val_loss: 23513442.0000\n",
      "Epoch 134/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23418936.0000 - val_loss: 23508038.0000\n",
      "Epoch 135/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 23412750.0000 - val_loss: 23502730.0000\n",
      "Epoch 136/150\n",
      "68/68 [==============================] - 169s 2s/step - loss: 23408228.0000 - val_loss: 23497602.0000\n",
      "Epoch 137/150\n",
      "68/68 [==============================] - 164s 2s/step - loss: 23403590.0000 - val_loss: 23492360.0000\n",
      "Epoch 138/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23400086.0000 - val_loss: 23487202.0000\n",
      "Epoch 139/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23460238.0000 - val_loss: 23482368.0000\n",
      "Epoch 140/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23390928.0000 - val_loss: 23477814.0000\n",
      "Epoch 141/150\n",
      "68/68 [==============================] - 158s 2s/step - loss: 23387240.0000 - val_loss: 23472906.0000\n",
      "Epoch 142/150\n",
      "68/68 [==============================] - 163s 2s/step - loss: 23382776.0000 - val_loss: 23468362.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/150\n",
      "68/68 [==============================] - 155s 2s/step - loss: 23378702.0000 - val_loss: 23464002.0000\n",
      "Epoch 144/150\n",
      "68/68 [==============================] - 168s 2s/step - loss: 23374050.0000 - val_loss: 23459704.0000\n",
      "Epoch 145/150\n",
      "68/68 [==============================] - 172s 3s/step - loss: 23370736.0000 - val_loss: 23455370.0000\n",
      "Epoch 146/150\n",
      "68/68 [==============================] - 157s 2s/step - loss: 23366596.0000 - val_loss: 23451190.0000\n",
      "Epoch 147/150\n",
      "68/68 [==============================] - 162s 2s/step - loss: 23362108.0000 - val_loss: 23446876.0000\n",
      "Epoch 148/150\n",
      "68/68 [==============================] - 157s 2s/step - loss: 23358196.0000 - val_loss: 23442890.0000\n",
      "Epoch 149/150\n",
      "68/68 [==============================] - 159s 2s/step - loss: 23354966.0000 - val_loss: 23439004.0000\n",
      "Epoch 150/150\n",
      "68/68 [==============================] - 160s 2s/step - loss: 23351316.0000 - val_loss: 23435212.0000\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = ModelCheckpoint(\n",
    "    'best_model.keras', \n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "def data_generator(data, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            x_batch = data[i:i+batch_size]\n",
    "            yield x_batch, x_batch  \n",
    "\n",
    "train_gen = data_generator(train_data, batch_size=16)\n",
    "val_gen = data_generator(val_data, batch_size=16)\n",
    "\n",
    "train_steps = len(train_data) // 16\n",
    "val_steps = len(val_data) // 16\n",
    "\n",
    "history = vae.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=val_steps,\n",
    "    epochs=150,\n",
    "    callbacks=[checkpoint_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0168fd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.50000006]\n",
      "  [0.5000001 ]\n",
      "  [0.50000006]\n",
      "  ...\n",
      "  [0.505936  ]\n",
      "  [0.505936  ]\n",
      "  [0.505936  ]]\n",
      "\n",
      " [[0.5000003 ]\n",
      "  [0.50000024]\n",
      "  [0.50000036]\n",
      "  ...\n",
      "  [0.505936  ]\n",
      "  [0.505936  ]\n",
      "  [0.505936  ]]\n",
      "\n",
      " [[0.5000031 ]\n",
      "  [0.5000034 ]\n",
      "  [0.5000028 ]\n",
      "  ...\n",
      "  [0.505936  ]\n",
      "  [0.505936  ]\n",
      "  [0.505936  ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.50000185]\n",
      "  [0.5000017 ]\n",
      "  [0.5000017 ]\n",
      "  ...\n",
      "  [0.505936  ]\n",
      "  [0.505936  ]\n",
      "  [0.505936  ]]\n",
      "\n",
      " [[0.50000167]\n",
      "  [0.50000167]\n",
      "  [0.500002  ]\n",
      "  ...\n",
      "  [0.505936  ]\n",
      "  [0.505936  ]\n",
      "  [0.505936  ]]\n",
      "\n",
      " [[0.505936  ]\n",
      "  [0.505936  ]\n",
      "  [0.505936  ]\n",
      "  ...\n",
      "  [0.505936  ]\n",
      "  [0.505936  ]\n",
      "  [0.505936  ]]], shape=(1024, 2048, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def generate_music(prior, decoder, n_samples):\n",
    "    \"\"\"\n",
    "    This function should compute generate new samples of spectograms/music from the generative model,\n",
    "    according to the above instructions.\n",
    "    The function takes the prior distribution, decoder and number of samples as inputs, which\n",
    "    should be used to generate the spectogram's data.\n",
    "    The function should then return the batch of generated spectogram data.\n",
    "    \"\"\"\n",
    "    z = prior.sample(n_samples)\n",
    "    return decoder(z).mean()\n",
    "\n",
    "n_samples = 1\n",
    "sm = generate_music(prior, decoder, n_samples)\n",
    "print(sm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c41535f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00316952  0.0148301  -0.01453933 ... -0.00591496  0.00612776\n",
      "  0.01863101]\n"
     ]
    }
   ],
   "source": [
    "FIXED_MIN_VAL = -80  \n",
    "FIXED_MAX_VAL = 0 \n",
    "\n",
    "def inverse_normalize_spectrogram(normalized_spectrogram):\n",
    "    return normalized_spectrogram * (FIXED_MAX_VAL - FIXED_MIN_VAL) + FIXED_MIN_VAL\n",
    "\n",
    "def stft_to_audio(tensor, output_path, sr=22050):\n",
    "    tensor_np = tensor.numpy() if isinstance(tensor, tf.Tensor) else tensor\n",
    "    y_reconstructed = librosa.griffinlim(tensor_np[:, :, 0])\n",
    "    # y_reconstructed = inverse_normalize_spectrogram(y_reconstructed)\n",
    "    print(y_reconstructed)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(stft_dB, sr=sample_rate, hop_length=hop_length, x_axis='time', y_axis='linear')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f'new')\n",
    "    plt.tight_layout()\n",
    "    sf.write(output_path, y_reconstructed, sr)\n",
    "stft_to_audio(sm[0], 'new.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b24c99c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
